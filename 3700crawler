#!/usr/bin/env python3

import sys, select, argparse, socket, ssl
from html.parser import HTMLParser

# CONSTANTS 
DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

# STATUS CODES
OK = 'HTTP/1.1 200 OK'
BAD_REQUEST = 'HTTP/1.1 400 Bad Request'
FOUND = 'HTTP/1.1 302 Found'
FORBIDDEN = 'HTTP/1.1 403 Forbidden'
NOT_FOUND = 'HTTP/1.1 404 Not Found'
SERVICE_UNAVAILABLE = 'HTTP/1.1 503 Service Unavailable'


class Crawler:
    
    # Crawl with one URL and response at a time
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

        self.secure_socket = None
        self.visited = ['/accounts/logout/']
        self.frontier = []
        self.flags = []

        self.sessionID = ''
        self.csrftoken = ''
        self.csrfmiddlewaretoken = ''

        self.current_url = ''
        self.current_status = ''
        self.current_headers = {}
        self.current_body = ''
    
    # Open and connect over TLS TCP socket
    def open_secure_socket(self):
        # Create socket 
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        # Wrap socket and set timeout
        SSL_CONTEXT = ssl.create_default_context()
        secure_socket = SSL_CONTEXT.wrap_socket(mysocket, server_hostname=self.server)
        secure_socket.settimeout(3)

        # Set as crawler socket
        self.secure_socket = secure_socket

        # Connect 
        self.secure_socket.connect((self.server, self.port))

    # Format and send a GET request with the given URL
    def send_get_request(self, url):
        if not isinstance(url, str):
            sys.exit(f'TypeError: send_get_request expects a string, not {type(url)}')
        
        # Set the current url 
        self.current_url = url

        # Format the request
        request = f"GET {self.current_url} HTTP/1.1\r\nHost: {self.server}\r\ncookie: csrftoken={self.csrftoken}; sessionid={self.sessionID}\r\n\r\n"
        
        # print("Request to %s:%d\n" % (self.server, self.port), file=sys.stderr)
        # print(request, file=sys.stderr)

        # Send the request
        self.secure_socket.send(request.encode('ascii'))

        # Receive the response and start the parsing cascade
        self.receive_headers()

    # Format and send POST request (expects fields as a dict)
    def send_post_request(self, path, fields):
        # Build the payload
        body = ''
        for k in fields:
            body += (k + '=' + fields[k] + '&') 
        body = body[:-1] + '\r\n\r\n'

        # Calculate the content length
        content_length = len(body.encode('ascii'))

        # Format the request
        request =  f"POST {path} HTTP/1.1\r\nHost: {self.server}\r\ncontent-length: {content_length}\r\ncontent-type: application/x-www-form-urlencoded\r\ncookie: csrftoken={self.csrftoken}; sessionid={self.sessionID}\r\n\r\n" + body

        # print("Request to %s:%d\n" % (self.server, self.port), file=sys.stderr)
        # print(request, file=sys.stderr)

        # Send the request 
        self.secure_socket.send(request.encode('ascii'))

        # Receive the response and start the parsing cascade
        return self.receive_headers()

    # Log into Fakebook (special GET and POST request protocol)
    def log_in(self):
        self.send_get_request('/accounts/login/')
        self.send_post_request(
            '/accounts/login/', 
            {'username':self.username, 
            'password':self.password,
            'csrfmiddlewaretoken':self.csrfmiddlewaretoken,
            'next':''})

    # Receive and process incoming headers
    def receive_headers(self):
        buf_size = 1024
        response = ''
        data = b''

        # Continue reading until the start of the body
        while not b'\r\n\r\n' in data:
            try:
                data = self.secure_socket.recv(buf_size)
                data_split = data.split(b'\r\n\r\n')
                # Only extract the headers
                headers = data_split[0]
                # If we receive some of the body, save it for later
                if len(data_split) == 2:
                    self.current_body = data_split[1].decode('ascii')
                response += headers.decode('ascii')
            except socket.error:
                # print("ERROR in receive_headers: " + str(socket.error), file=sys.stderr)
                exit()

        # Set status and headers
        response = response.split('\r\n')
        self.current_status = response[0]
        self.set_headers(response)

        # Handle the status before proceeding with body
        self.handle_status()

    # Receive and process incoming body based on content-length
    def receive_body(self, buf_size):
        response = ''

        try:
            data = self.secure_socket.recv(buf_size)
            response += data.decode('ascii')
        except socket.error:
            # print("ERROR in receive_headers: " + str(socket.error), file=sys.stderr)
            exit()

        # Set the body (if some of it was already received, just append)
        self.current_body += response 

    # Determine how to proceed based on status
    # We've already eliminated the URLs for 404 and 403, so they've already been dealt with
    def handle_status(self):
        if self.current_status == BAD_REQUEST:
            self.handle_BAD_REQUEST()

        elif self.current_status == OK:
            self.handle_OK()
            
        elif self.current_status == FOUND:
            self.handle_FOUND()

        elif self.current_status == SERVICE_UNAVAILABLE:
            self.handle_SERVICE_UNAVAILABLE()

    # If a bad request is encountered, something bad happened in the client -- exit
    def handle_BAD_REQUEST(self):
        # print('HTTP/1.1 400 Bad Request', file=sys.stderr)
        exit()

    # The program can receive the rest of the body and start the parsing cascade
    def handle_OK(self):
        # We can visit the current URL 
        self.visited += self.current_url,

        # Receive the rest of the body
        self.receive_body(int(self.current_headers['Content-Length']))

        self.parse_html()

        # Extract the session ID and CSRF token
        if 'sessionid' in self.current_headers['Cookies'].keys():
            self.sessionID = self.current_headers['Cookies']['sessionid']
        if 'csrftoken' in self.current_headers['Cookies'].keys():
            self.csrftoken = self.current_headers['Cookies']['csrftoken']

    # Try request again with new URL in Location header
    def handle_FOUND(self):
        # Receive the rest of the body
        self.receive_body(int(self.current_headers['Content-Length']))

        # Extract the session ID and CSRF token
        if 'sessionid' in self.current_headers['Cookies'].keys():
            self.sessionID = self.current_headers['Cookies']['sessionid']
        if 'csrftoken' in self.current_headers['Cookies'].keys():
            self.csrftoken = self.current_headers['Cookies']['csrftoken']

        # Extract Location
        location = self.current_headers['Location']

        # Try GET again with Location
        self.send_get_request(location)

    # Retry request for the URL until successful
    def handle_SERVICE_UNAVAILABLE(self):
        self.send_get_request(self.current_url)

    # Parse incoming headers and store them
    def set_headers(self, headers):
        self.current_headers['Cookies'] = {}
        for h in headers:
            # current_reponse needs unique keys, so store each 'Set-Cookie' header in 
            # a 'Cookies' dictionary 
            if 'Set-Cookie: ' in h:
                # Isolate cookie value in 'Set-Cookie: ' header
                cookie = h.replace('Set-Cookie: ','')
                # Desired cookie value is bounded by '<cookie_type>=' and '; <attributes>'
                # so split by ';' to isolate '<cookie_type>=<cookie_value>' 
                # then split by '=' to get pair ['<cookie_type>','<cookie_value>']
                pair = cookie.split(';')[0].split('=')
                # Set the cookie key and value in the cookies dictionary in current_headers 
                self.current_headers['Cookies'][pair[0]] = pair[1]
            elif ':' in h:
                # Split each header => Header: value --> ['Header','value']
                pair = h.split(': ')
                # Set current_response header key and associated value from pair above
                self.current_headers[pair[0]] = pair[1]

    # Parse incoming HTML and extract valuable information
    def parse_html(self):
        parser = MyHTMLParser()
        parser.feed(self.current_body)
        if parser.data['csrf']:
            self.csrfmiddlewaretoken = parser.data['csrf']
        if parser.data['flags']:
            for f in parser.data['flags']:
                if f not in self.flags:
                    self.flags += f,
        if parser.data['urls']:
            for u in parser.data['urls']:
                u = self.truncate_target(u)
                if (u not in self.visited) and (u not in self.frontier) and (len(u) > 0):
                    self.frontier.append(u)

    # Ensure target domain
    def truncate_target(self, url):
        if url.startswith('/'):
            return url
        elif url.startswith('https://proj5.3700.network/'):
            return url.replace('https://proj5.3700.network','')
        else:
            return ''



    def run(self):
        self.open_secure_socket()
        self.log_in()
       
        while len(self.frontier) > 0:
            if len(self.flags) == 5: break
            next = self.frontier.pop(0)
            self.send_get_request(next)
        #     print("Size of frontier: " + str(len(self.frontier)), file=sys.stderr)
        #     print('\nNumber of pages visited: ' + str(len(self.visited)), file=sys.stderr)
        #     print('Flags: ' + str(self.flags) + "\n", file=sys.stderr)

        # print("----------------- Metrics ----------------", file=sys.stderr)
        # print("Size of frontier: " + str(len(self.frontier)))
        # print("Flags: " + str(self.flags), file=sys.stderr)
        # print('Final number of pages visited: ' + str(len(self.visited)), file=sys.stderr)
        # print("------------------------------------------", file=sys.stderr)

        for f in self.flags:
            print(f, file=sys.stdout)
        
# Custom HTML parser from HTMLParser
class MyHTMLParser(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        # Store relevant information
        self.data = {
            'csrf':'',
            'urls':[],
            'flags':[]
        }

    def handle_starttag(self, tag, attrs):
        if tag == 'input':
            tag_dict = dict(attrs)
            # Extract csrfmiddlewaretoken
            if 'csrfmiddlewaretoken' in tag_dict.values():
                self.data['csrf'] = tag_dict['value']
        
        if tag == 'a':
            tag_dict = dict(attrs)
            # Extract urls
            if 'href' in tag_dict.keys():
                self.data['urls'] += tag_dict['href'],

    def handle_data(self, data):
        if data.startswith('FLAG: '):
            # Extract flags
            self.data['flags'] += data.replace('FLAG: ',''),

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
